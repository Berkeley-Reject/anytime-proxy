# Stream Processing

## Event Stream

In general, the **stream** refers to data that is incrementally made available over time. The **event stream** is a data management mechanism that could process unbounded data. The **event** is a small, self-contained, immutable object containing the details of something that happened at some point in time. The topic or stream is a group of related events. The event is generated by a producer, and then processed by multiple consumers.

### Messaging System

The messaging system notifies consumers about new events. The producer sends a message containing the event, which is then pushed to consumers. The messaging system designed for event processing reduces the overhead of the polling operations of the consumers.

The messaging system could use direct network communication between producers and consumers without going through intermediary nodes. (e.g. UDP multicast, brokerless messaging libraries, HTTP or RPC requests) However, the system requires the application code to be aware of the possibility of message loss and assumes that producers and consumers are constantly online.

#### Message Broker

The messaging system could store the events received from the producers in a centralized database, which is a message broker. The message broker stores the events in a buffer, and deletes the event when it has been delivered to its consumers. The consumers subscribe to a subset of topics and asynchronously receive the events by reading from the broker. The system tolerates crashed clients and persists events in the memory or disk.

The message broker uses acknowledgements to ensure that the message is not lost. The client must expliticly tell the broker when it has finished processing a message so that the broker could remove it from the queue. If the connection to a client is closed without acknowledgement, it assumes that the message was not processed and tries to deliver it to another consumer.

- Load balancing: Each message in the same topic is delivered to one of the consumers. The pattern is useful when the messages are expensive to process, thus the system could parallelize the processing.
- Fan-out: Each message in the same topic is delivered to all of the consumers. The pattern allows several independent consumers to process the same broadcast of messages, without affecting each other.

### Partitioned Logs

In the tranditional message broker, the consumer is destructive if the acknowledgment causes the received messages to be deleted from the broker. The log-based message broker (Apache Kafka, Amazon Kinesis Streams, etc.) combines the durable storage of the database with the low-latency notification facilities of messaging. The log is an append-only sequence of records on disk. The producer sends a message by appending it to the end of the log, and a consumer receives messages by reading the log sequentially.

To scale to higher throughput, the log could be partitioned, and each topic could hold a group of partitions. Different partitions could be hosted on different machines, making each partition a separate log. Within each partition, the broker assigns a monotonically increasing sequence number  to every message, but there is no ordering guarantee across different partitions. Therefore, all messages that need to be ordered consistently need to be routed to the same partition.

- Load balancing: The log-based message broker could assign entire partitions to nodes in the consumer group. However, if a single message is slow to process, the node holds up the processing of subsequent messages in that partition.
- Fan-out: The log-based message broker trivially supports fan-out messaging, because consumers could independently read the log without affecting each other.

The consumer processes messages sequentially, thus messages with an offset less than a consumer's current offset have been processed. The broker could record the consumer offsets instead of tracking acknowledgements for each message. If a consumer node fails, another node in the consumer group starts consuming messages at the last recorded offset.

## Database and Stream

### Change Data Capture

Non-trivial applications combine several different technologies (e.g. OLTP database, full-text index, data warehouse) to satisfy their requirements. Because the same or related data appears in several different places, they should be kept in consistent. **Change data capture** is the process of observing all data changes written to a database and extracting them to replicate to other systems. The log-based message broker is suited for transporting the change events from the source database to the derived systems, since it preserves the ordering of messages.

### Event Sourcing

**Event sourcing** is the process of storing all changes to the application state as a log of change events. The application logic is built on the basis of immutable events that are written to an append-only event log. Events are designed to reflect things that happened at the application level, rather than low-level state changes. The application replays the event log to derive the application state and persists the snapshot of the current state to reduce the log size.

### State, Stream, and Immutability

The sequence of events that caused the change of the application state is immutable. The application state is the result of the events that mutated it over time. In a database system, the content of the database could be viewed as a cache of the latest record values in the logs, but the distinction could be bridged with log compaction. Since the event log is immutable, several read-oriented representations with different schemas and access patterns could be derived from the event log.

- The consumers of the event log are asynchronous, thus the user might write to the log, then read from a log-derived view, and find that the write hasn't been reflected in the read view.
- If the events contains a high rate of updates and deletes on a comparatively small dataset, the immutable history might contain fragmentation. Therefore, the performance of compaction and garbage collection becomes crucial for operation robustness.
- The immutable data might be required to be deleted for legal reasons (e.g. GDPR). The history should be rewrited to pretend that the data was never written to the system.
